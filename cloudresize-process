#! /usr/bin/python


import os,sys, base64, time, pickle
from bootstrap import Bootstrap
from boto.sqs.connection import SQSConnection
from boto.s3.connection import S3Connection

def doResize ( imagedetail, imagepath ):
  print "Resizing"
  

global configuration
configuration = Bootstrap.bootstrap()

if configuration:
  sqsconn = SQSConnection(configuration.get('AWS', 'AWSID'), configuration.get('AWS', 'SECRETKEY'))
  s3conn = S3Connection(configuration.get('AWS', 'AWSID'), configuration.get('AWS', 'SECRETKEY'))
  queue = sqsconn.get_queue(configuration.get('AWS', 'QUEUE'))
  maxload = 2
  try:
    maxload = configuration.get('AWS', 'MAXLOAD')
  except:
    pass
  
  tempdir = "/tmp"
  try:
    tempdir = configuration.get('AWS', 'TEMPDIR')
  except:
    pass
  
  while True:
    # wait for load to drop
    while os.getloadavg()[0] > maxload:
      print "Waiting for load avg to drop"
      time.sleep(10)
    
    found = False
    try:
      message = queue.read(60)
      if message:
	found = True
	messagedetail = message.get_body()
	imagedetail = pickle.loads(messagedetail)
	bucketname = configuration.get('AWS', 'BUCKET')
	if 'bucket' in imagedetail:
	  bucketname = imagedetail['bucket']
	
	# download from bucket
	bucket = s3conn.get_bucket(bucketname)
	key = bucket.get_key( imagedetail['key'] )
	print "Downloading", imagedetail['key']
	
	key.get_contents_to_filename( os.path.join(tempdir,imagedetail['tempfilename'] ) )
	
	profiles = []
	
	for profile in profiles:
	  resizedfile = doResize( imagedetail, os.path.join(tempdir,imagedetail['tempfilename'] ), profile )
	
	  # upload
	  keydetail = ""
	  newkey = bucket.new_key(keydetail)
	  newkey.set_contents_from_file( resizedfile )
	  newkey.close()
	  close(resizedfile)
	  unlink(resizedfile)
	
	# successfully proccessed, delete old
	key.delete()
	message.delete()
    except Exception as error:
      print "Error",error
      pass
    break # for testing
    if not found:
      print "Waiting for messages in queue"
      time.sleep(60)